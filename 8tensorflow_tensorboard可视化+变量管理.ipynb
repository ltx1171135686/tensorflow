{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-519109a57d5b>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST.data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST.data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST.data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST.data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST.data', one_hot=True)\n",
    "\n",
    "batch_size = 100\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "n_test_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "#只要是用于权值，偏置\n",
    "def variable_summaries(var):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean',mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n",
    "    tf.summary.scalar('stddev',stddev)\n",
    "    tf.summary.scalar('max',tf.reduce_max(var))\n",
    "    tf.summary.scalar('min',tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogrom',var)   #直方图\n",
    "\n",
    "\n",
    "def weight_variable(shape,name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial,name=name)\n",
    "\n",
    "def bias_variable(shape,name):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial,name=name)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    #x 有4维。[batch, 长, 宽, 深度]\n",
    "    #W 过滤器4维。 [长, 宽, 输入深度, 输出深度]\n",
    "    #tf.nn.conv2d同样传入4个参数 conv2d(x,w,strides=[4个参数], padding='SAME)  strides步长。只涉及中间两个参数，左右/上下走\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "#池化层\n",
    "def max_pool_2x2(x):\n",
    "    #ksize=[1,x,y,1]\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0,Testing Accuracy 0.10123636345971715\n",
      "Iter 100,Testing Accuracy 0.4490181796659123\n",
      "Iter 200,Testing Accuracy 0.7181090932542628\n",
      "Iter 300,Testing Accuracy 0.827818180214275\n",
      "Iter 400,Testing Accuracy 0.8426909081502394\n",
      "Iter 500,Testing Accuracy 0.847490906607021\n",
      "Iter 600,Testing Accuracy 0.8490363630381498\n",
      "Iter 700,Testing Accuracy 0.8610909075086767\n",
      "Iter 800,Testing Accuracy 0.9440000011704185\n",
      "Iter 900,Testing Accuracy 0.9557818207957528\n",
      "Iter 1000,Testing Accuracy 0.9568000028350137\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('input'):\n",
    "    #定义两个placeholder\n",
    "    x = tf.placeholder(tf.float32, [None,784], name='x-input')\n",
    "    y = tf.placeholder(tf.float32, [None,10], name='y-output')\n",
    "    with tf.name_scope('x_image'):\n",
    "        x_image = tf.reshape(x, [-1,28,28,1],name='x-image')\n",
    "        \n",
    "with tf.name_scope('Conv1'):\n",
    "    with tf.name_scope('W_conv1'):\n",
    "        W_conv1 = weight_variable([5,5,1,32], name='W_conv1')\n",
    "    with tf.name_scope('b_conv1'):\n",
    "        b_conv1 = bias_variable([32], name='b_conv1')\n",
    "    \n",
    "    #池化层一般不算单独一层，放在卷积层理\n",
    "    with tf.name_scope('conv2d_1'):\n",
    "        conv2d_1 = conv2d(x_image, W_conv1) + b_conv1\n",
    "    with tf.name_scope('relu'):\n",
    "        h_conv1 = tf.nn.relu(conv2d_1)\n",
    "    with tf.name_scope('h_pool1'):\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "        \n",
    "with tf.name_scope('Conv2'):\n",
    "    with tf.name_scope('W_conv2'):\n",
    "        W_conv2 = weight_variable([5,5,32,64],name='W_conv2')\n",
    "    with tf.name_scope('b_conv2'):\n",
    "        b_conv2 = bias_variable([64], name='b_conv2')\n",
    "    \n",
    "    with tf.name_scope('conv2d_2'):\n",
    "        conv2d_2 = conv2d(h_pool1, W_conv2) + b_conv2\n",
    "    with tf.name_scope('relu'):\n",
    "        h_conv2 = tf.nn.relu(conv2d_2)\n",
    "    with tf.name_scope('h_pool2'):\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "        \n",
    "with tf.name_scope('fc1'):\n",
    "    with tf.name_scope('W_fc1'):\n",
    "        W_fc1 = weight_variable([7*7*64, 1024], name='W_fc1')\n",
    "    with tf.name_scope('b_fc1'):\n",
    "        b_fc1 = bias_variable([1024], name='b_fc1')\n",
    "    \n",
    "    with tf.name_scope('h_pool2_flat'):\n",
    "        h_pool_flat = tf.reshape(h_pool2, [-1,7*7*64],name='h_pool_flat')\n",
    "    \n",
    "    with tf.name_scope('wx_plus_b'):\n",
    "        wx_plus_b = tf.matmul(h_pool_flat,W_fc1) + b_fc1\n",
    "    with tf.name_scope('relu'):\n",
    "        h_fc1 = tf.nn.relu(wx_plus_b)\n",
    "    \n",
    "    #dropout\n",
    "    with tf.name_scope('keep_prob'):\n",
    "        keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    with tf.name_scope('h_fc1_drop'):\n",
    "        h_hc1_drop = tf.nn.dropout(h_fc1, keep_prob, name='h_fc1_drop')\n",
    "        \n",
    "with tf.name_scope('fc2'):\n",
    "    with tf.name_scope('W_fc2'):\n",
    "        W_fc2 = weight_variable([1024, 10], name='W_fc2')\n",
    "    with tf.name_scope('b_fc2'):\n",
    "        b_fc2 = bias_variable([10], name='b_fc2')    \n",
    "    \n",
    "    with tf.name_scope('wx_plus_b'):\n",
    "        wx_plus_b = tf.matmul(h_hc1_drop,W_fc2) + b_fc2\n",
    "    with tf.name_scope('relu'):\n",
    "        h_fc2 = tf.nn.relu(wx_plus_b)\n",
    "    \n",
    "    with tf.name_scope('softmax'):\n",
    "        prediction = tf.nn.softmax(h_fc2)\n",
    "        \n",
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=y))\n",
    "    #显示标量信息。用来可视化\n",
    "    tf.summary.scalar('cross_entropy',cross_entropy)\n",
    "    \n",
    "with tf.name_scope('train_step'):\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "    with tf.name_scope('accuracy_rate'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "        \n",
    "        #\n",
    "        tf.summary.scalar('accuracy',accuracy)  #没必要使用variable_summaries.上面的权重有很多。可以看下均值等。这个一次就只有一个\n",
    "\n",
    "        \n",
    "        \n",
    "#合并所有的merge\n",
    "merged = tf.summary.merge_all()  #合并上面所有的summary\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter('loss/train',sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('loss/test',sess.graph)\n",
    "    for i in range(1001):\n",
    "        batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        sess.run(train_step,feed_dict={x:batch_xs, y:batch_ys,keep_prob:0.5})\n",
    "        \n",
    "        #记录训练集计算的参数\n",
    "        summary = sess.run(merged, feed_dict={x:batch_xs, y:batch_ys, keep_prob:1.0})\n",
    "                \n",
    "        #上面两步可以合并写\n",
    "        #summary,_ = summary = sess.run([merged,train_step], feed_dict={x:batch_xs, y:batch_ys, keep_prob:1.0})\n",
    "        \n",
    "        #写到train_writer和test_writer中。  tf.summary.scalar会记录每一个值。 写到磁盘中则使用 tf.summary.FileWriter     \n",
    "        train_writer.add_summary(summary,i)\n",
    "        \n",
    "        batch_xs,batch_ys = mnist.test.next_batch(batch_size)\n",
    "        \n",
    "        #记录训练集计算的参数\n",
    "        summary = sess.run(merged, feed_dict={x:batch_xs, y:batch_ys, keep_prob:1.0})\n",
    "        \n",
    "        test_writer.add_summary(summary,i)\n",
    "        \n",
    "        test_acc = 0\n",
    "        if i%100 == 0:\n",
    "            \n",
    "            for batch in range(n_test_batch):\n",
    "                batch_test_xs,batch_test_ys = mnist.test.next_batch(batch_size)\n",
    "                test_acc += sess.run(accuracy, feed_dict={x: batch_test_xs, y: batch_test_ys,keep_prob:1})\n",
    "            \n",
    "            print(\"Iter \"+ str(i) + \",Testing Accuracy \" + str(test_acc/(batch+1))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#笔记\n",
    "#    当cpu和GPU版本的tensorflow同时安装时。需要使用Anaconda的GPU的cmd.运行tensorboard。\n",
    "#    tensorboard==1.13.1可能报错。回滚安装1.12.2 \n",
    "\n",
    "#   变量管理的优点：\n",
    "#          ①当层数变多时，变量管理方便阅读\n",
    "#          ②可视化时，层次非常明显\n",
    "#          ③使用训练好的神经网络进行推导时，可以直接调用"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
